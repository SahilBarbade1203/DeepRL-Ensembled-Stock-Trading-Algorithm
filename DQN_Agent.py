{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMhLhoDj2VYXWPUaSvTWoB9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"pL2relLCPewe"},"outputs":[],"source":["#DQN Model\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import os\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset,DataLoader\n","import torch.optim as optim\n","\n","import yfinance as yf\n","from collections import deque\n","import random\n","import math\n","from tqdm import tqdm\n","\n","class DQN(nn.Module):\n","    def __init__(self, state_dim, action_dim):\n","        super(DQN, self).__init__()\n","        self.fc1 = nn.Linear(state_dim, 64)\n","        self.fc2 = nn.Linear(64, 32)\n","        self.fc3 = nn.Linear(32, 8)\n","        self.fc4 = nn.Linear(8, action_dim)\n","\n","    def forward(self, x):\n","        x = torch.relu(self.fc1(x))\n","        x = torch.relu(self.fc2(x))\n","        x = torch.relu(self.fc3(x))\n","        output = torch.softmax(self.fc4(x), dim=-1)\n","        return output\n","\n","\n","# Main Agent Work\n","\n","class DQN_Agent:\n","    def __init__(self, state_dim, is_eval=False, model_name=\"\"):\n","        self.model_type = \"DQN\"\n","        self.state_dim = state_dim\n","        self.action_dim = 3  # hold, sell, and buy\n","        self.memory = deque(maxlen=100)\n","        self.buffer_size = 60\n","\n","        self.gamma = 0.95\n","        self.epsilon = 1.0  # initial exploration rate\n","        self.epsilon_min = 0.01  # minimum exploration rate\n","        self.epsilon_decay = 0.995  # decrease exploration rate as the agent becomes good at trading\n","        self.is_eval = is_eval\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","        # self.model = torch.load(f'{model_name}.pth') if is_eval else self.create_model().to(self.device)\n","        # Load the model with proper device mapping\n","        if is_eval:\n","            self.model = self.create_model().to(self.device)\n","\n","            if self.device.type == 'cpu':\n","                self.model.load_state_dict(torch.load(f'{model_name}.pth', map_location=torch.device('cpu')))\n","\n","            else:\n","                self.model.load_state_dict(torch.load(f'{model_name}.pth'))\n","            self.model.eval()\n","        else:\n","            self.model = self.create_model().to(self.device)\n","\n","\n","        self.optimizer = optim.Adam(self.model.parameters(), lr=0.01)\n","        self.loss_fn = nn.MSELoss()\n","\n","    def create_model(self):\n","        return DQN(self.state_dim, self.action_dim)\n","\n","    def remember(self, state, action, reward, next_state, done):\n","        self.memory.append((state, action, reward, next_state, done))\n","\n","    def act(self, state):\n","        if not self.is_eval and np.random.rand() <= self.epsilon:\n","            return random.randrange(self.action_dim)\n","\n","        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n","        with torch.no_grad():\n","            options = self.model(state)\n","        return torch.argmax(options[0]).item()\n","\n","    def experience_replay(self, batch_size):\n","        mini_batch = random.sample(self.memory, min(len(self.memory), batch_size))\n","\n","        for state, action, reward, next_state, done in mini_batch:\n","            state = torch.tensor(state, dtype=torch.float32).to(self.device)\n","            next_state = torch.tensor(next_state, dtype=torch.float32).to(self.device)\n","\n","            target = reward\n","            if not done:\n","                target = reward + self.gamma * torch.max(self.model(next_state)).item()\n","\n","            # Predict Q-values for the current state\n","            target_f = self.model(state).detach().cpu().numpy()\n","\n","            # Ensure target_f is a 1D array and update the action index\n","            target_f = target_f.squeeze()  # Remove any extra dimensions\n","            target_f[action] = target\n","\n","            target_f = torch.tensor(target_f, dtype=torch.float32).to(self.device).unsqueeze(0)\n","            state = torch.tensor(state, dtype=torch.float32).to(self.device)\n","\n","            self.optimizer.zero_grad()\n","            loss = nn.MSELoss()(self.model(state), target_f)\n","            loss.backward()\n","            self.optimizer.step()\n","\n","        if self.epsilon > self.epsilon_min:\n","            self.epsilon *= self.epsilon_decay\n","\n","        return loss.item()\n"]}]}