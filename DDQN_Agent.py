{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMth/VZad/hKk6AjC5Q+I8G"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"hfrQ2HWtP2ha"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import random\n","from collections import deque\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import os\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset,DataLoader\n","import torch.optim as optim\n","\n","import yfinance as yf\n","from collections import deque\n","import random\n","import math\n","from tqdm import tqdm\n","\n","# DDQN Model\n","class DDQN(nn.Module):\n","    def __init__(self, state_dim, action_dim):\n","        super(DDQN, self).__init__()\n","        self.fc1 = nn.Linear(state_dim, 64)\n","        self.fc2 = nn.Linear(64, 32)\n","        self.fc3 = nn.Linear(32, 8)\n","        self.fc4 = nn.Linear(8, action_dim)\n","\n","    def forward(self, x):\n","        x = torch.relu(self.fc1(x))\n","        x = torch.relu(self.fc2(x))\n","        x = torch.relu(self.fc3(x))\n","        output = torch.softmax(self.fc4(x), dim=-1)\n","        return output\n","\n","# DDQN Agent\n","class DDQN_Agent:\n","    def __init__(self, state_dim, tau=0.0001, is_eval=False, model_name=\"\"):\n","        self.model_type = \"DDQN\"\n","        self.state_dim = state_dim\n","        self.action_dim = 3  # hold, sell, and buy\n","        self.memory = deque(maxlen=100)\n","        self.buffer_size = 60\n","\n","        self.gamma = 0.95\n","        self.epsilon = 1.0  # initial exploration rate\n","        self.epsilon_min = 0.01  # minimum exploration rate\n","        self.epsilon_decay = 0.995  # decrease exploration rate as the agent becomes good at trading\n","        self.is_eval = is_eval\n","        self.tau = tau\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","        if is_eval:\n","            self.model = self.create_model().to(self.device)\n","            self.model_target = self.create_model().to(self.device)\n","\n","            if self.device.type == 'cpu':\n","                self.model.load_state_dict(torch.load(f'{model_name}.pth', map_location=torch.device('cpu')))\n","                self.model_target.load_state_dict(torch.load(f'{model_name}_target.pth', map_location=torch.device('cpu')))\n","            else:\n","                self.model.load_state_dict(torch.load(f'{model_name}.pth'))\n","                self.model_target.load_state_dict(torch.load(f'{model_name}_target.pth'))\n","            self.model.eval()\n","            self.model_target.eval()\n","        else:\n","            self.model = self.create_model().to(self.device)\n","            self.model_target = self.create_model().to(self.device)\n","            self.model_target.load_state_dict(self.model.state_dict())\n","\n","        self.optimizer = optim.Adam(self.model.parameters(), lr=0.01)\n","        self.loss_fn = nn.MSELoss()\n","\n","    def create_model(self):\n","        return DDQN(self.state_dim, self.action_dim)\n","\n","    def remember(self, state, action, reward, next_state, done):\n","        self.memory.append((state, action, reward, next_state, done))\n","\n","    def act(self, state):\n","        if not self.is_eval and np.random.rand() <= self.epsilon:\n","            return random.randrange(self.action_dim)\n","\n","        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n","        with torch.no_grad():\n","            options = self.model(state)\n","        return torch.argmax(options[0]).item()\n","\n","    def experience_replay(self, batch_size):\n","        if len(self.memory) < batch_size:\n","            return  # Not enough samples in memory\n","\n","        mini_batch = random.sample(self.memory, min(len(self.memory), batch_size))\n","\n","        for state, action, reward, next_state, done in mini_batch:\n","            state = torch.tensor(state, dtype=torch.float32).to(self.device)\n","            next_state = torch.tensor(next_state, dtype=torch.float32).to(self.device)\n","\n","            # Double DQN: Select the action using the model, evaluate using the target network\n","            target = reward\n","            if not done:\n","                best_action = torch.argmax(self.model(next_state)).item()\n","                target = reward + self.gamma * self.model_target(next_state)[0][best_action].item()\n","\n","            target_f = self.model(state).detach().cpu().numpy()\n","            target_f = target_f.squeeze()\n","            target_f[action] = target\n","\n","            target_f = torch.tensor(target_f, dtype=torch.float32).to(self.device).unsqueeze(0)\n","\n","            self.optimizer.zero_grad()\n","            loss = self.loss_fn(self.model(state), target_f)\n","            loss.backward()\n","            self.optimizer.step()\n","\n","        if self.epsilon > self.epsilon_min:\n","            self.epsilon *= self.epsilon_decay\n","\n","        # Update the target model with the new method\n","        self.update_model_target()\n","\n","        return loss.item()\n","\n","    def update_model_target(self):\n","        # Get the state_dict of the current model and the target model\n","        model_weights = self.model.state_dict()\n","        model_target_weights = self.model_target.state_dict()\n","\n","        # Update the weights of the target model\n","        for key in model_weights:\n","            model_target_weights[key] = self.tau * model_weights[key] + (1 - self.tau) * model_target_weights[key]\n","\n","        # Load the updated weights into the target model\n","        self.model_target.load_state_dict(model_target_weights)"]}]}