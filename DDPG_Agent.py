{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOjZ2r0BzwbMGoffe2lNrD3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"fommRcihQHjz"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import os\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset,DataLoader\n","import torch.optim as optim\n","\n","import yfinance as yf\n","from collections import deque\n","import random\n","import math\n","from tqdm import tqdm\n","\n","#DDPG Model\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import random\n","from collections import deque\n","\n","# DDPG Model\n","class ActorNetwork(nn.Module):\n","    def __init__(self, state_dim, action_dim):\n","        super(ActorNetwork, self).__init__()\n","        self.fc1 = nn.Linear(state_dim, 64)\n","        self.fc2 = nn.Linear(64, 32)\n","        self.fc3 = nn.Linear(32, 8)\n","        self.fc4 = nn.Linear(8, action_dim)\n","\n","    def forward(self, x):\n","        x = torch.relu(self.fc1(x))\n","        x = torch.relu(self.fc2(x))\n","        x = torch.relu(self.fc3(x))\n","        output = torch.tanh(self.fc4(x))\n","        return output\n","\n","class CriticNetwork(nn.Module):\n","    def __init__(self, state_dim, action_dim):\n","        super(CriticNetwork, self).__init__()\n","        self.fc1 = nn.Linear(state_dim + action_dim, 64)\n","        self.fc2 = nn.Linear(64, 32)\n","        self.fc3 = nn.Linear(32, 8)\n","        self.fc4 = nn.Linear(8, 1)\n","\n","    def forward(self, x):\n","        x = torch.relu(self.fc1(x))\n","        x = torch.relu(self.fc2(x))\n","        x = torch.relu(self.fc3(x))\n","        output = self.fc4(x)\n","        return output\n","\n","class OrnsteinUhlenbeckNoise:\n","    \"OU noise generator class, used to add OU noise to actions.\"\n","    def __init__(self, size, mu=0, sigma=0.1, theta=0.15):\n","        self.mu = mu * np.ones(size)\n","        self.sigma = sigma\n","        self.theta = theta\n","        self.size = size\n","        self.reset()\n","\n","    def reset(self):\n","        \"Resets noise to mean.\"\n","        self.state = self.mu.copy()\n","\n","    def sample(self):\n","        \"Returns next value generated in process.\"\n","        dx = self.theta * (self.mu - self.state) + self.sigma * np.random.randn(self.size)\n","        self.state += dx\n","        return self.state.copy()\n","\n","# DPMG Agent\n","class DDPG_Agent:\n","    def __init__(self, state_dim, tau=0.0001, is_eval=False, model_name=\"\"):\n","        self.model_type = \"DDPG\"\n","        self.state_dim = state_dim\n","        self.action_dim = 3  # hold, sell, and buy\n","        self.memory = deque(maxlen=100)\n","        self.buffer_size = 60\n","\n","        self.gamma = 0.95\n","        self.is_eval = is_eval\n","        self.tau = tau\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.noise_generator = OrnsteinUhlenbeckNoise(size=self.action_dim, mu=0)\n","\n","        # if is_eval:\n","        #     self.model = self.create_model().to(self.device)\n","        #     self.model_target = self.create_model().to(self.device)\n","\n","        #     if self.device.type == 'cpu':\n","        #         self.model.load_state_dict(torch.load(f'{model_name}.pth', map_location=torch.device('cpu')))\n","        #         self.model_target.load_state_dict(torch.load(f'{model_name}_target.pth', map_location=torch.device('cpu')))\n","        #     else:\n","        #         self.model.load_state_dict(torch.load(f'{model_name}.pth'))\n","        #         self.model_target.load_state_dict(torch.load(f'{model_name}_target.pth'))\n","        #     self.model.eval()\n","        #     self.model_target.eval()\n","        # else:\n","        self.actor_model = self.create_actor_model().to(self.device)\n","        self.critic_model = self.create_critic_model().to(self.device)\n","\n","        self.actor_model_target = self.create_actor_model().to(self.device)\n","        self.actor_model_target.load_state_dict(self.actor_model.state_dict())\n","\n","        self.critic_model_target = self.create_critic_model().to(self.device)\n","        self.critic_model_target.load_state_dict(self.critic_model.state_dict())\n","\n","        self.actor_optimizer = optim.Adam(self.actor_model.parameters(), lr=0.01)\n","        self.critic_optimizer = optim.Adam(self.critic_model.parameters(), lr = 0.01)\n","\n","        self.loss_fn = nn.MSELoss()\n","\n","    def create_actor_model(self):\n","        return ActorNetwork(self.state_dim, self.action_dim)\n","\n","    def create_critic_model(self):\n","        return CriticNetwork(self.state_dim, self.action_dim)\n","\n","    def remember(self, state, action, reward, next_state, done):\n","        self.memory.append((state, action, reward, next_state, done))\n","\n","    def act(self, state, noise = None):\n","        with torch.no_grad():\n","            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n","            action = self.actor_model(state).squeeze(0).detach().cpu().numpy()\n","            if noise is not None:\n","                return np.clip(action + noise, a_min=0, a_max=2)\n","        return action\n","\n","\n","    def experience_replay(self, batch_size):\n","        if len(self.memory) < batch_size:\n","            return  # Not enough samples in memory\n","\n","        mini_batch = random.sample(self.memory, min(len(self.memory), batch_size))\n","\n","        for state, action, reward, next_state, done in mini_batch:\n","            state = torch.tensor(state, dtype=torch.float32).to(self.device)\n","            next_state = torch.tensor(next_state, dtype=torch.float32).to(self.device)\n","            action = torch.tensor(action, dtype=torch.float32).to(self.device)\n","\n","            current_action_q  = self.critic_model(torch.cat((state, action), dim=1))\n","\n","            with torch.no_grad():\n","              next_state_q = self.critic_model_target(torch.cat((next_state, self.actor_model_target(next_state)), dim = 1))\n","              target_q = reward + self.gamma * next_state_q * (1 - done)\n","\n","            self.critic_optimizer.zero_grad()\n","            critic_loss = self.loss_fn(current_action_q, target_q)\n","            critic_loss.backward()\n","            nn.utils.clip_grad_norm_(self.critic_model.parameters(), 1000)\n","            self.critic_optimizer.step()\n","\n","            current_action_q = self.critic_model(torch.cat((state, self.actor_model(state)), dim=1))\n","            actor_loss = -(current_action_q).mean()\n","\n","            self.actor_optimizer.zero_grad()\n","            actor_loss.backward()\n","            nn.utils.clip_grad_norm_(self.actor_model.parameters(), 1000)\n","            self.actor_optimizer.step()\n","\n","        # Update the target model with the new method\n","        self.update_model_target(self.critic_model, self.critic_model_target)\n","        self.update_model_target(self.actor_model, self.actor_model_target)\n","\n","\n","    def update_model_target(self, current, target):\n","        # Get the state_dict of the current model and the target model\n","        model_weights = current.state_dict()\n","        model_target_weights = target.state_dict()\n","\n","        # Update the weights of the target model\n","        for key in model_weights:\n","            model_target_weights[key] = self.tau * model_weights[key] + (1 - self.tau) * model_target_weights[key]\n","\n","        # Load the updated weights into the target model\n","        target.load_state_dict(model_target_weights)"]}]}